---
title: 'Titanic Survival Analysis'
author: 'Jacob Salway'
output:
  html_document:
    number_sections: true
    toc: true
    toc_float: true
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
# knitr::opts_chunk$set(out.height="\\textheight", out.width="\\textwidth")
```

# Intro

In this report, I will analyse the Titanic dataset from the challenge hosted on Kaggle. I will do:

* Cleaning and missing value imputation
* Exploratory data analysis
* Feature engineering
* Prediction

## Load data

```{r, message=F}
# load packages
library(ggplot2)
library(gridExtra)
library(dplyr)
library(tidyverse)
library(randomForest)
library(reshape2)
library(janitor)

theme_set(theme_minimal())
```

After loading the packages, let's take a look at the data.

```{r, message=F, warning=F}
train <- read.csv('data/train.csv')
test <- read.csv('data/test.csv')

# for separation later
train$source <- 'train'
test$source <- 'test'

# combine
full <- bind_rows(train, test)

# clean names
full <- clean_names(full)

# check data
str(full)
```

Let's just reclassify some existing variables to factors.

```{r}
cols = c('survived', 'embarked', 'source')
for (i in cols) {
  full[,i] <- as.factor(full[,i])
}
```

## Missing values

Let's take a look at what's missing.

```{r}
colSums(is.na(full))
colSums(full == '')
```

We can see that we have a lot of missing age and cabin values, one missing fare value and two missing embarked values. The missing survival values are due to the test dataset not having a survival factor given. For fare and embarked, we'll just impute with the median and mode respectively.

```{r}
full$fare[is.na(full$fare)] <- median(full$fare, na.rm=T)

# ugly workaround for mode of factor
full$embarked[full$embarked == ''] <- tail(names(sort(table(full$embarked))), 1)
```

To impute the age, we're going to use a slightly better method than just using the median. Instead, we're going to sample from the existing distribution of ages.

```{r}
freq = data.frame(table(full$age))
freq = freq[freq$Var1 != "",]
freq$prob <- freq$Freq / sum(freq$Freq)
size = nrow(full[is.na(full$age),]) # num rows to sample

sampled = sample(freq$Var1, prob=freq$prob, replace=T, size=size)
full$age[is.na(full$age)] <- sampled
```

Let's compare the distribution of the sampled ages to the real ages.

```{r}
full %>%
  ggplot(aes(x=age)) +
  geom_histogram() +
  labs(x='Age', y='Frequency', title='Age')

t = data.frame(sampled)
t$sampled = as.numeric(t$sampled)
t %>%
  ggplot(aes(x=sampled)) +
  geom_histogram() +
  labs(x='Age', y='Frequency', title='Sampled age')
```

Let's check that we sorted out those null values.

```{r}
colSums(is.na(full))
colSums(full == '')

full <- droplevels(full) # drop unused factor levels
```

The only column with any null or empty values left is the cabin value, however this would be very difficult to impute so we're going to leave it.

## Data columns

* `passenger_id` is just the ID of the row.
* `survived` is a binary variable indicating survival
  * **1 = Survived**
  * **0 = Didn't survive**
* `pclass` is passenger class and indicates the socio-economic status of the passenger.
  * **1 = Upper class**
  * **2 = Middle class**
  * **3 = Lower class**
* `name`, `sex` and `age` are all as their name suggests.
* `sib_sp` indicates the number of the passenger's siblings and spouses.
* `parch` indicates the number of the passenger's parents and children.
* `ticket` is the ticket number of the passenger.
* `fare` is the passenger's fare.
* `cabin` is the cabin number of the passenger.
* `embarked` is the port the passenger embarked from.
  * **C = Cherbourg**
  * **Q = Queenstown**
  * **S = Southampton**

# Feature engineering

Feature engineering is using our knowledge and intuition to create new features or transform existing ones to provide additional information for our model to analyse. Let's begin by looking at a correlation matrix of the major numerical variables.

```{r}
t = full %>%
  filter(source=='train') %>%
  select(age, fare, parch, pclass, sib_sp, survived) %>%
  mutate(survived=as.numeric(survived)) # need survival as factor for other charts

t.mat = round(cor(t), 2)
t.matlong = melt(t.mat)

t.matlong %>%
  ggplot(aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  geom_text(aes(Var1, Var2, label=value), color='black', size=4) +
  labs(x='Variable', y='Variable', title='Correlation heatmap') +
  scale_fill_gradient2(low='blue', high='red', mid='white', midpoint=0, limit=c(-1, 1))
```

Between variables other than survival, we can see there is a significant correlation between `sib_sp` and `parch`, and `pclass` and `fare`. For other variables and survival, we can see that `pclass` and `fare` have a significant correlation.

## Family size

Considering that `sib_sp` and `parch` are all counts of the passenger's family, let's just pool that all together into a `family_size` variable.

```{r}
full$family_size <- full$sib_sp + full$parch + 1 # add one for passenger
```

## Title

If we look at a sample of the the names in the dataset, we can see that each name has a title. This will definitely be useful information for our model so let's extract it.

```{r}
head(full$name)

# split string and fetch second element which is the title
full$title <- sapply(full$name, function(x) {strsplit(x, split='[,.]')[[1]][2]})

# delete empty space before title
full$title <- sub(' ', '', full$title)

table(full$title)
```

We can see there are a lot of interesting titles. It's likely that these titles with small counts are all statistically significant, and this many separate categories would be problematic for our model, so let's compress the ones under a certain frequency.

```{r}
title_freq = data.frame(table(full$title))
title_freq = title_freq[title_freq$Freq < 10,] # ten is a small number

full$title[full$title %in% title_freq$Var1] <- 'Other'

# change to factor
full$title <- as.factor(full$title)

table(full$sex, full$title)
```

If we look at the distribution of the titles across genders, we can see most of the *noble* titles and all the masters were male.

## Deck

There's also some useful information in the `cabin` variable.

```{r}
full$cabin[full$cabin != ''][1:20]
```

We can see that data takes the pattern of **[A-Z][0-9][0-9]**. The letter in front of the number indicates the deck that the passenger's cabin was on. Some passengers have multiple cabins but they're mostly the same deck so we can just extract the first letter of cabin as a new factor. To deal with all the missing values in cabin we'll just replace them with a new group **M**.

```{r}
full$cabin[full$cabin == ''] <- "M" # replace null cabin values with m
full$deck <- as.factor(sapply(full$cabin, function(x) strsplit(x, NULL)[[1]][[1]])) # first char of first string
```

Let's look at our data set now.

```{r}
str(full)
```

# Exploratory analysis

We're now going to do some exploratory data analysis. The first thing we're going to look at is the distribution of some of the major variables. Firstly let's look at the distribution of ages and survival proportion.

```{r}
# filter to only training data so we can see survival values
train <- full %>%
  filter(source=='train')

sex_dist = train %>%
  ggplot(aes(x=sex, fill=survived)) +
  geom_bar(position='stack') +
  labs(x='Sex', y='Count')

sex_surv_prop = train %>%
  ggplot(aes(x=sex, fill=survived)) +
  geom_bar(position='fill') +
  labs(x='Sex', y='Proportion')

grid.arrange(sex_dist, sex_surv_prop, ncol=2)
```

We can see that there were nearly double the number of males, and that only 25% of males survived while nearly 75% of females survived. We can definitely say that sex will be a major variable in our model.

Let's look at the distribution of some other major numerical variables now.

```{r}
g1 = train %>%
  ggplot(aes(x=fare)) +
  geom_histogram(binwidth=10) +
  labs(x='Fare', y='Frequency')

g1p = train %>%
  ggplot(aes(x=fare, fill=survived)) +
  geom_histogram(binwidth=10, position='fill') +
  labs(x='Fare', y='Proportion')

grid.arrange(g1, g1p, ncol=2)

g2 = train %>%
  ggplot(aes(x=age)) +
  geom_histogram(binwidth=3) +
  labs(x='Age', y='Frequency')

g2p = train %>%
  ggplot(aes(x=age, fill=survived)) +
  geom_histogram(binwidth=3, position='fill') +
  labs(x='Age', y='Proportion')

grid.arrange(g2, g2p, ncol=2)

g3 = train %>%
  ggplot(aes(x=family_size)) +
  geom_histogram(binwidth=1) +
  labs('Family size', y='Frequency')

g3p = train %>%
  ggplot(aes(x=family_size, fill=survived)) +
  geom_histogram(binwidth=1, position='fill') +
  labs(x='Family size', y='Proportion')

grid.arrange(g3, g3p, ncol=2)
```

We can see that the distution of `family_size` is very right skewed and `age` is slightly right skewed. `fare` has a large clustering because of our mean value imputation.

Now, let's take a look at the survival proportions in `embarked` and `pclass`.

```{r}
g1p = train %>%
  ggplot(aes(x=embarked, fill=survived)) +
  geom_bar(position='fill') +
  labs(x='Embarked', y='Proportion')

g2p = train %>%
  ggplot(aes(x=pclass, fill=survived)) +
  geom_bar(position='fill') +
  labs(x='Passenger class', y='Proportion')

grid.arrange(g1p, g2p, ncol=2)
```

It seems like there is a distinct relationship between `embarked` and `pclass` with what seem to be statistically significant proportion differences. Let's test that.

```{r}
# embarked
(tbl = table(train$embarked, train$survived))
(chi = chisq.test(tbl))
chi$p.value < 0.01
# pclass
(tbl = table(train$pclass, train$survived))
(chi = chisq.test(tbl))
chi$p.value < 0.01
```

We can see that our p-values are less than our level of significance, so we reject the null hypothesis and accept the alternative hypothesis to conclude that `embarked` and `pclass` are associated to survival.

For our last graph, let's see how title relates to survival with gender.

```{r}
g1p = train %>%
  filter(sex=='female') %>%
  ggplot(aes(x=title, fill=survived)) +
  geom_bar(position='fill', show.legend=F) +
  labs(x='Title', y='Proportion', title='Female')

g2p = train %>%
  filter(sex=='male') %>%
  ggplot(aes(x=title, fill=survived)) +
  geom_bar(position='fill', show.legend=F) +
  labs(x='Title', y='Proportion', title='Male')

grid.arrange(g1p, g2p, ncol=2)
```

We can see that females are the most likely to survive with a single women having a nearly 75% survival rate. Males on the other hand are much more likely not to survive, especially men with the title of Mr. Notably, men with a title of Master had a greater than 50% survival rate.

# Model

## Random forest

For our machine learning model we're going to use a random forest classification model.

```{r}
# just training data
train <- full %>%
  filter(source=='train')
test <- full %>%
  filter(source=='test')

# model
formula = survived ~ pclass + sex + age + sib_sp + parch + fare + embarked + family_size + title + deck 
classifier <- randomForest(formula, data=train)

err = data.frame(classifier$err.rate)
err = err %>% mutate(trees=1:n())
err = melt(err, id.vars='trees')

err %>%
  ggplot(aes(x=trees, y=value)) +
  geom_line(aes(color=variable)) +
  labs(x='Trees', y='Error rate (%)', 'Error rates')
```

This graph shows us the error rates for the random forest prediction. The red line represents the overall error rate, and the green and blue lines are the error rate for `survived = 0` and `survived = 1` predictions respectively. We can see that we predict death more accurately compared to survival. 

## Variable importance

Let's see the variable importance graph looks like.

```{r}
importance = importance(classifier)
importance = data.frame(variables = row.names(importance), importance=round(importance[,'MeanDecreaseGini'], 2))

importance %>%
  ggplot(aes(x=reorder(variables, importance), y=importance, fill=importance)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(x='Importance', y='Variable', title='Feature importance')
```

We can see that `title`, `fare`, `sex` and `age` were the most important variables for the random forest model. 

## Performance measures

Let's take a look at the confusion matrix and some typical machine learning scores.

```{r}
con_mat = data.frame(classifier$confusion)[,c('X0', 'X1')]
con_mat

tn = con_mat[,'X0'][1]
fn = con_mat[,'X0'][2]
fp = con_mat[,'X1'][1]
tp = con_mat[,'X1'][2]

accuracy = (tp + tn) / sum(con_mat)
precision = tp / (tp + fp)
sensitivity = tp / (tp + fn)
f1 = 2*(1/((1/precision) + (1/sensitivity)))
specificity = tn / (tn + fp)

labels = c('accuracy', 'precision', 'sensitivity', 'f1', 'specificity')
stats = c(accuracy, precision, sensitivity, f1, specificity)

df = data.frame(labels=labels, stats=stats)
df
```

We can see that our model had an accuracy of **0.83** meaning it correctly labelled passengers 83% of the time. A precision of **0.81** means that 81% of the passengers we predicted to survive did actually survive, and a sensitivity of **0.71** means that of the people who did survive, we correctly predicted 71% of them. The F1 score of **0.76** is the harmonic mean of precision and sensitivity. A specificity of **0.90**. means that out of all people who died, we correctly predicted 90% of them.

# Prediction

Let's actually use the model on the test data now.

```{r}
pred = predict(classifier, test)
output = data.frame(PassengerId = test$passenger_id, Survived=pred)
write.csv(output, 'titanic_pred.csv', row.names=F, quote=F)
```

I've submitted these predictions to Kaggle and got a score of 0.76, so reasonably close to our training accuracy score.